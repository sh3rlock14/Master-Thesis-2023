{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Download, Install, Import :blush: :cd:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install wandb==0.13.3 -qqq\n","!pip install monai -qqq\n","!pip install pytorch_lightning -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip uninstall kornia -y\n","!pip install kornia"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Libraries :books: "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:46.007223Z","iopub.status.busy":"2023-03-28T15:31:46.006824Z","iopub.status.idle":"2023-03-28T15:31:56.773617Z","shell.execute_reply":"2023-03-28T15:31:56.772451Z","shell.execute_reply.started":"2023-03-28T15:31:46.007173Z"},"trusted":true},"outputs":[{"data":{"text/html":["        <script type=\"text/javascript\">\n","        window.PlotlyConfig = {MathJaxConfig: 'local'};\n","        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n","        if (typeof require !== 'undefined') {\n","        require.undef(\"plotly\");\n","        requirejs.config({\n","            paths: {\n","                'plotly': ['https://cdn.plot.ly/plotly-2.18.0.min']\n","            }\n","        });\n","        require(['plotly'], function(Plotly) {\n","            window._Plotly = Plotly;\n","        });\n","        }\n","        </script>\n","        "]},"metadata":{},"output_type":"display_data"}],"source":["\n","## misc\n","import os\n","import argparse\n","import yaml\n","import tempfile\n","import glob\n","\n","from datetime import datetime, timedelta\n","from typing import Any, Callable, Dict, Hashable, no_type_check, List, Mapping, Optional, Sequence, Tuple, TypeVar, Union\n","from typing_extensions import Literal\n","from types import SimpleNamespace\n","\n","## logging\n","import wandb\n","\n","## math, dl, vision\n","import numpy as np\n","\n","import torchvision\n","from kornia.losses.hausdorff import HausdorffERLoss3D\n","\n","import torch\n","import torch.nn as nn\n","from torch import rand, cat, stack, amax, where, sigmoid\n","import torch.nn.functional as F\n","from torch.cuda import is_available\n","from torch.nn import Dropout3d\n","from torch.nn.init import kaiming_normal_, normal_, constant_\n","\n","\n","## torchmetrics\n","from torchmetrics import Metric\n","from torchmetrics.functional.classification import multilabel_stat_scores\n","from torchmetrics.functional.classification.dice import _dice_compute\n","from torchmetrics.utilities.enums import AverageMethod, MDMCAverageMethod\n","\n","## pytorch lightning \n","from pytorch_lightning import seed_everything\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning import Trainer\n","from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint, LearningRateMonitor, EarlyStopping\n","from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme, CustomBarColumn, ProcessingSpeedColumn\n","\n","\n","#from torch.utils.data import Dataset, DataLoader\n","from pytorch_lightning import LightningDataModule\n","\n","\n","\n","## aesthetics\n","from rich.progress import ProgressColumn, Task, TextColumn\n","from rich.text import Text\n","from rich.style import Style\n","from rich.console import RenderableType\n","\n","## MONAI\n","from monai.apps import DecathlonDataset\n","from monai.config.type_definitions import NdarrayOrTensor, NdarrayTensor\n","from monai.config import DtypeLike, KeysCollection\n","from monai.data import DataLoader, decollate_batch\n","from monai.data.meta_obj import get_track_meta\n","from monai.data.meta_tensor import MetaTensor\n","from monai.inferers import sliding_window_inference\n","from monai.losses import DiceLoss, FocalLoss, DiceFocalLoss, GeneralizedDiceLoss, GeneralizedDiceFocalLoss\n","from monai.metrics import DiceMetric\n","from monai.networks.nets import SegResNet, SegResNetVAE\n","from monai.utils import TraceKeys, set_determinism, UpsampleMode\n","from monai.utils.type_conversion import convert_data_type, convert_to_dst_type, convert_to_tensor, get_equivalent_dtype\n","\n","\n","## data-viz\n","import plotly\n","import plotly.express as px\n","plotly.offline.init_notebook_mode (connected = True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Env Variables & Dirs :open_file_folder:"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:56.791020Z","iopub.status.busy":"2023-03-28T15:31:56.790534Z","iopub.status.idle":"2023-03-28T15:31:56.797672Z","shell.execute_reply":"2023-03-28T15:31:56.796543Z","shell.execute_reply.started":"2023-03-28T15:31:56.790982Z"},"trusted":true},"outputs":[],"source":["os.makedirs('/kaggle/working/wandb/', exist_ok=True)\n","\n","#os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n","os.environ[\"WANDB_DIR\"] = os.path.abspath(\"/kaggle/working/\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:56.800991Z","iopub.status.busy":"2023-03-28T15:31:56.800542Z","iopub.status.idle":"2023-03-28T15:31:56.806848Z","shell.execute_reply":"2023-03-28T15:31:56.805824Z","shell.execute_reply.started":"2023-03-28T15:31:56.800955Z"},"trusted":true},"outputs":[],"source":["task = \"Task01_BrainTumour\"\n","config_dir = \"/kaggle/input/brats-configurations/\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:56.776357Z","iopub.status.busy":"2023-03-28T15:31:56.775961Z","iopub.status.idle":"2023-03-28T15:31:56.788772Z","shell.execute_reply":"2023-03-28T15:31:56.787498Z","shell.execute_reply.started":"2023-03-28T15:31:56.776316Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["## Set seeds for reproducibility and avoid 'workers' to apply\n","## the same DataAugmentation twice\n","my_seed = 0\n","set_determinism(seed=my_seed)\n","seed_everything(my_seed, workers=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# src :gear:"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:56.808723Z","iopub.status.busy":"2023-03-28T15:31:56.808293Z","iopub.status.idle":"2023-03-28T15:31:56.820627Z","shell.execute_reply":"2023-03-28T15:31:56.819541Z","shell.execute_reply.started":"2023-03-28T15:31:56.808685Z"},"trusted":true},"outputs":[],"source":["from torch import tensor\n","Tensor = TypeVar(\"Tensor\", bound=tensor)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Custom Metrics :chart_with_upwards_trend:"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.069537Z","iopub.status.busy":"2023-03-28T15:31:57.069145Z","iopub.status.idle":"2023-03-28T15:31:57.089166Z","shell.execute_reply":"2023-03-28T15:31:57.087875Z","shell.execute_reply.started":"2023-03-28T15:31:57.069498Z"},"trusted":true},"outputs":[],"source":["class Val_Dice(Metric):\n","    \"\"\"\n","    Custom Val Dice Metric mutuated from MONAI Dice Metric, but with the possibility\n","    to be extended to multiclass/multilabel segmentation.\n","    In the original I wasn't able to find the correct combination of parameters to get the\n","    desired reduction (on batch and classes) for multidim data.\n","    The same for torchmetrics Dice metric.\n","\n","    So, I decided to rebuilt it from scratch, based on the pred/gt stats.\n","    \"\"\"\n","    is_differentiable: bool = False\n","    higher_is_better: bool = True\n","    full_state_update: bool = False\n","\n","    @no_type_check\n","    def __init__(\n","        self,\n","        zero_division: int = 0,\n","        num_classes: Optional[int] = None,\n","        threshold: float = 0.5,\n","        average: Optional[Literal[\"micro\", \"macro\", \"weighted\", \"none\"]] = \"micro\",\n","        mdmc_average: Optional[str] = \"samplewise\",\n","        ignore_index: Optional[int] = None,\n","        top_k: Optional[int] = None,\n","        multiclass: Optional[bool] = None,\n","        **kwargs: Any,\n","    ) -> None:\n","        super().__init__(**kwargs)\n","        allowed_average = (\"micro\", \"macro\", \"weighted\", \"samples\", \"none\", None)\n","        if average not in allowed_average:\n","            raise ValueError(f\"The `average` has to be one of {allowed_average}, got {average}.\")\n","\n","        _reduce_options = (AverageMethod.WEIGHTED, AverageMethod.NONE, None)\n","        if \"reduce\" not in kwargs:\n","            kwargs[\"reduce\"] = AverageMethod.MACRO if average in _reduce_options else average\n","        if \"mdmc_reduce\" not in kwargs:\n","            kwargs[\"mdmc_reduce\"] = mdmc_average\n","\n","        self.reduce = average\n","        self.mdmc_reduce = mdmc_average\n","        self.num_classes = num_classes\n","        self.threshold = threshold\n","        self.multiclass = multiclass\n","        self.ignore_index = ignore_index\n","        self.top_k = top_k\n","\n","        default: Callable = lambda: []\n","        reduce_fn: Optional[str] = \"cat\"\n","\n","        for s in (\"tp\", \"fp\", \"tn\", \"fn\", \"sup\"):\n","            self.add_state(s, default=default(), dist_reduce_fx=reduce_fn)\n","        \n","        self.average = average\n","        self.zero_division = zero_division\n","    \n","    @no_type_check\n","    def update(self, preds: Tensor, target: Tensor) -> None:\n","        \"\"\"Update state with predictions and targets.\"\"\"\n","        stats = multilabel_stat_scores(preds, target, self.num_classes , average=self.average, multidim_average=self.mdmc_reduce)\n","        tp, fp, tn, fn, sup = torch.unbind(stats.reshape(preds.shape[0], self.num_classes, 5), dim=-1) # 3/4 dpending on num_classes\n","        \n","        # Update states\n","        self.tp.append(tp)\n","        self.fp.append(fp)\n","        self.tn.append(tn)\n","        self.fn.append(fn)\n","        self.sup.append(sup)\n","    \n","    @no_type_check\n","    def _get_final_stats(self) -> Tuple[Tensor, Tensor, Tensor, Tensor,Tensor]:\n","        \"\"\"Performs concatenation on the stat scores if neccesary, before passing them to a compute function.\"\"\"\n","        tp = torch.cat(self.tp) if isinstance(self.tp, list) else self.tp\n","        fp = torch.cat(self.fp) if isinstance(self.fp, list) else self.fp\n","        tn = torch.cat(self.tn) if isinstance(self.tn, list) else self.tn\n","        fn = torch.cat(self.fn) if isinstance(self.fn, list) else self.fn\n","        sup = torch.cat(self.sup) if isinstance(self.sup, list) else self.sup\n","        return tp, fp, tn, fn, sup\n","        \n","    @no_type_check\n","    def compute(self) -> Tensor:\n","        \"\"\"Compute metric.\"\"\"\n","        tp, fp, _, fn, sup = self._get_final_stats()\n","        res =  _dice_compute(tp, fp, fn, self.average, self.mdmc_reduce, self.zero_division)\n","        return res"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Pytorch Lightning Modules :zap:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Module :zap: :file_cabinet:"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:56.824323Z","iopub.status.busy":"2023-03-28T15:31:56.823570Z","iopub.status.idle":"2023-03-28T15:31:56.839005Z","shell.execute_reply":"2023-03-28T15:31:56.837863Z","shell.execute_reply.started":"2023-03-28T15:31:56.824287Z"},"trusted":true},"outputs":[],"source":["# @title brats2018\n","\n","class DatasetModule(LightningDataModule):\n","    \n","    def __init__(\n","        self,\n","        data_path: str,\n","        dataset: str,\n","        train_batch_size: int = 1,\n","        val_batch_size: int = 2,\n","        num_workers: int = 0,\n","        download: bool = False,\n","        cache_rate = 0.0,\n","        transform: Optional[dict] = None,\n","        classes: int = 3,\n","        **kwargs,\n","    ):\n","        super().__init__()\n","        self.data_path = data_path \n","        self.dataset = dataset # task \n","        self.train_batch_size = train_batch_size \n","        self.val_batch_size = val_batch_size \n","        self.num_workers = num_workers\n","        self.download = download\n","        self.cache_rate = cache_rate\n","        self.transform = transform\n","        self.args = kwargs # If needed try SimpeNameSpace(**kwargs)\n","\n","\n","    def setup(self, stage: Optional[str] = None) -> None:\n","        ### DATASETS ###\n","        if self.dataset.lower() == \"task01_braintumour\":\n","            if stage in [\"fit\", \"validation\", None]:\n","                self.train_dataset = DecathlonDataset(\n","                    root_dir = self.data_path,\n","                    task = self.dataset,\n","                    transform = self.transform['training'] if self.transform is not None else None,\n","                    section = \"training\",\n","                    download = self.download,\n","                    cache_rate = self.cache_rate,\n","                    **self.args\n","                )\n","\n","        self.val_dataset = DecathlonDataset(\n","            root_dir = self.data_path,\n","            task = self.dataset,\n","            transform = self.transform['validation'] if self.transform is not None else None,\n","            section = \"validation\",\n","            download = self.download,\n","            cache_rate = self.cache_rate,\n","            **self.args\n","        )\n","  \n","      \n","    ### DATALOADERS ###\n","    def train_dataloader(self) -> DataLoader:\n","        return DataLoader(self.train_dataset, batch_size = self.train_batch_size, shuffle=True, num_workers = self.num_workers)\n","    \n","    def val_dataloader(self) -> DataLoader:\n","        return DataLoader(self.val_dataset, batch_size = self.val_batch_size, shuffle=False, num_workers = self.num_workers)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Experiment Module :zap: :brain: "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.093177Z","iopub.status.busy":"2023-03-28T15:31:57.092709Z","iopub.status.idle":"2023-03-28T15:31:57.135365Z","shell.execute_reply":"2023-03-28T15:31:57.134244Z","shell.execute_reply.started":"2023-03-28T15:31:57.093143Z"},"trusted":true},"outputs":[],"source":["# @title ThesisExperiment\n","\n","from pytorch_lightning import LightningModule\n","\n","from torch.optim import SGD, Adam, RMSprop\n","from torch import cat\n","\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","class ThesisExperiment(LightningModule):\n","    def __init__(self,\n","                 model_name,\n","                 model,\n","                 criterion_name,\n","                 criterion,\n","                 optimizer: str,\n","                 params: dict) -> None:\n","        \n","        super(ThesisExperiment, self).__init__()\n","        \n","        \n","        # FOR MANUAL OPTIMIZATION set to False\n","        self.automatic_optimization = True\n","        self.model_name = model_name.lower()\n","        self.model = model\n","        self.criterion_name = criterion_name\n","        self.criterion = criterion\n","        self.optimizer = optimizer.lower()\n","        self.params = SimpleNamespace(**params)\n","        self.curr_device = None\n","        self.hold_graph = False\n","        self.val_dice = Val_Dice(num_classes=params['outClasses'],\n","                                 average='none',\n","                                 mdmc_average='samplewise') # here params is still a dict\n","        self.hausdorff_loss = HausdorffERLoss3D(alpha=params['Hausdorff']['alpha'],\n","                                               k=params['Hausdorff']['k'],\n","                                               reduction=params['Hausdorff']['reduction']\n","                                              )\n","        self.hausdorff_lambda = 1.0\n","        self.hd_losses = []\n","        self.dsc_losses = []\n","        \n","        try:\n","            self.hold_graph = self.params['retain_first_backpass']\n","        except:\n","            pass\n","        \n","    \n","    def forward(self, input: Tensor, **kwargs) -> Tensor:\n","        return self.model(input, **kwargs)\n","    \n","    def training_step(self, batch, batch_idx):\n","        #ipdb.set_trace(context=4)\n","        if self.params.inModalities == 4:\n","            input_tensor, target = batch['image'].as_tensor(), batch['label'].as_tensor()\n","        elif self.params.inModalities == 1:\n","            input_tensor, target = batch[f'image_{self.params.modality}'].as_tensor(), batch['label'].as_tensor()\n","            \n","        \n","        output = self.forward(input_tensor) # output shape:  torch.Size([2, 3, 3, 128, 128, 128]) B, DS, K, H, W, D\n","        \n","        if self.model_name == \"segresnetvae\":\n","            output, vae_reg_loss, vae_mse_loss = output\n","\n","\n","        # BEFORE REFACTORING FOR DEEP SUPERVISION\n","        \"\"\"\n","        if self.criterion_name == \"dice\":\n","            loss_batch = self.criterion(output, target).mean(dim=0).squeeze() # reduction over batches\n","            loss = loss_batch.mean() # reduction over classes\n","        elif self.criterion_name == \"focal\":\n","            loss_batch = self.criterion(output, target).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","            loss = loss_batch.mean() #reduction over classes\n","        elif self.criterion_name in [\"dicefocal\",\"gen_dicefocal\"]:\n","            dl_batch = self.criterion['dice'](output, target).mean(dim=0).squeeze() # reduction over batches\n","            fl_batch = self.criterion['focal'](output, target).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","            loss_batch = (self.criterion['lambda_dice'] * dl_batch) + (self.criterion['lambda_focal'] * fl_batch)\n","            loss = loss_batch.mean() # reduction over classes \n","        \"\"\"\n","        \n","        ## AFTER REFACTORING FOR DEEP SUPERVISION\n","        if self.model_name == \"attentionunet\":\n","            if not self.params.deep_supervision:\n","                pass # as before (after debug copy the commented lines in this block)\n","            else:\n","                loss, weights = 0., 0.\n","                for i in range(output.shape[1]):\n","                    dl_batch = self.criterion['dice'](output[:,i], target).mean(dim=0).squeeze() # reduction over batches\n","                    fl_batch = self.criterion['focal'](output[:,i], target).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","                    loss_batch = (self.criterion['lambda_dice'] * dl_batch) + (self.criterion['lambda_focal'] * fl_batch)\n","                    loss += loss_batch.mean() * 0.5 ** i # reduction over classes\n","                    weights += 0.5 ** i\n","                loss = loss/weights\n","        else:\n","            if self.criterion_name == \"dice\":\n","                loss_batch = self.criterion(output, target).mean(dim=0).squeeze() # reduction over batches\n","                loss = loss_batch.mean() # reduction over classes\n","            elif self.criterion_name == \"focal\":\n","                loss_batch = self.criterion(output, target).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","                loss = loss_batch.mean() #reduction over classes\n","            elif self.criterion_name in [\"dicefocal\",\"gen_dicefocal\"]:\n","                dl_batch = self.criterion['dice'](output, target).mean(dim=0).squeeze() # reduction over batches\n","                fl_batch = self.criterion['focal'](output, target).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","                loss_batch = (self.criterion['lambda_dice'] * dl_batch) + (self.criterion['lambda_focal'] * fl_batch)\n","                loss = loss_batch.mean() # reduction over classes \n","            \n","        \n","        \n","        if self.model_name == \"segresnetvae\":\n","            loss = loss + 0.1 * vae_mse_loss + 0.1 * vae_reg_loss\n","            \n","            \n","            \n","        ## NEW: compute the 3D (differentiable) HausdorffLoss for each of the 3 labels and add to the final loss\n","        if self.params.use_hausdorff:\n","            if not self.params.deep_supervision:\n","                pass\n","            else:\n","                \n","                ## FOR NOW: compute HD on all three seg_pred\n","                ## it could be worthwhile to compute HD onto WT (and TC) only -> ET is usually far from being convex and it\n","                ## could only be a problem for the loss descent\n","                label_seg_pred = [TC_seg_pred, WT_seg_pred, ET_seg_pred] = output[:,0].unbind(dim=1) # need to select the first 'head' from DS'output\n","                label_seg_gt = [TC_seg_gt, WT_seg_gt, ET_seg_gt] = target.unbind(dim=1)\n","                \n","                HD_losses = torch.zeros(3)\n","                \n","                for lbl in range(len(label_seg_pred)):\n","                    ## Avoid using 'hard values' when computing the \n","                    #hdl = self.hausdorff_loss(val_act_threshold(label_seg_pred[lbl][:,None,:]), label_seg_gt[lbl][:,None,:]) # shape of :math:`(B, C, D, H, W), here C == 1`\n","                    ## use insted the probability map\n","                    hdl = self.hausdorff_loss(label_seg_pred[lbl][:,None,:], label_seg_gt[lbl][:,None,:]) # shape of :math:`(B, C, D, H, W), here C == 1`\n","                    \n","                    \n","                    HD_losses[lbl] = hdl\n","                    \n","                hd_loss = HD_losses.mean()\n","            \n","        \n","        ## Log the losses\n","        if self.params.outClasses == 4:\n","            loss_names = [f'{self.criterion_name}_loss_train', 'bg_loss_train', 'TC_loss_train', 'WT_loss_train', 'ET_loss_train', 'hausdorff_loss']\n","            losses = loss_batch.squeeze().tolist()\n","        else:\n","            loss_names = [f'{self.criterion_name}_loss_train', 'TC_loss_train', 'WT_loss_train', 'ET_loss_train', 'hausdorff_loss']\n","            losses = loss_batch.squeeze().tolist()\n","            \n","        \n","        self.log_dict(\n","            dict(\n","                zip(loss_names,\n","                    [loss.item(), *losses, hd_loss.item()]\n","                   )\n","            ),\n","            batch_size = self.params.val_batch_size,\n","            on_step = False,\n","            on_epoch = True,\n","            rank_zero_only = True\n","        )\n","        \n","        ## Update the losses list to update 'on train epoch end' the `hausdorff_lambda` parameter\n","        self.hd_losses.append(hd_loss.item())\n","        self.dsc_losses.append(loss.item())\n","        \n","        \n","        # Combine the hd_loss and the dicefocal loss\n","        loss = (hd_loss * self.hausdorff_lambda) + loss\n","        \n","        \n","        return loss\n","    \n","    \n","    def on_train_epoch_end(self):\n","        ## Update the `hausdorff_lambda` parameter and clear the losses list\n","        ## for the next epoch\n","        hd_loss_mean = np.mean(self.hd_losses)\n","        dsc_loss_mean = np.mean(self.dsc_losses)\n","        r = hd_loss_mean/dsc_loss_mean\n","        \n","        self.hausdorff_lambda = r\n","        self.hd_losses.clear()\n","        self.dsc_losses.clear()\n","        \n","    \n","    def validation_step(self, batch, batch_idx):\n","        ## During this step the HD is not computed\n","        ## \n","\n","        if self.params.inModalities == 4:\n","            input_tensor, target = batch['image'], batch['label']\n","        elif self.params.inModalities == 1:\n","            input_tensor, target = batch[f'image_{self.params.modality}'], batch['label']\n","        \n","        \n","        val_output = sliding_window_inference(\n","            inputs=input_tensor,\n","            roi_size=(240, 240, 160),\n","            sw_batch_size=4,\n","            predictor=self.model,\n","            overlap=0.5)\n","    \n","        \n","        batch['pred'] = val_output\n","        batch['pred_meta_dict'] = val_output.as_dict(key='pred')['pred_meta_dict']\n","        \n","        ## Compute the validation losses\n","        if self.criterion_name == \"dice\":\n","            loss_batch = self.criterion(val_output.as_tensor(), target.as_tensor()).mean(dim=0).squeeze() # reduction over batches\n","            loss = loss_batch.mean() # reduction over classes\n","        elif self.criterion_name == \"focal\":\n","            loss_batch = self.criterion(val_output.as_tensor(), target.as_tensor()).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","            loss = loss_batch.mean() #reduction over classes\n","        elif self.criterion_name in [\"dicefocal\", \"gen_dicefocal\"]:\n","            dl_batch = self.criterion['dice'](val_output.as_tensor(), target.as_tensor()).mean(dim=0).squeeze() # reduction over batches\n","            fl_batch = self.criterion['focal'](val_output.as_tensor(), target.as_tensor()).mean(dim=[0, 2, 3, 4]) # reduction over batches\n","            loss_batch = (self.criterion['lambda_dice'] * dl_batch) + (self.criterion['lambda_focal'] * fl_batch)\n","            loss = loss_batch.mean() # reduction over classes \n","        \n","        \n","        ## Activate and Discretize the outputs\n","        ## Maybe it is possible to compute `val_act_threshold` directly on batch, w/o decollating\n","        ## this way there should be no need to stack the output again\n","        val_output = [val_act_threshold(i) for i in decollate_batch(val_output.as_tensor())]\n","        \n","    \n","        ## Update the valdation metrics\n","        # BEFORE:\n","        #self.val_dice.update(batch['pred'].as_tensor(), target.as_tensor())\n","        # AFTER:\n","        self.val_dice.update(torch.stack(val_output), target.as_tensor())\n","        \n","        \n","        ## Log the losses\n","        if self.params.outClasses == 4: # the bg is explicitely computed\n","            loss_names = [f'{self.criterion_name}_loss_val', 'bg_loss_val', 'TC_loss_val', 'WT_loss_val', 'ET_loss_val']\n","            losses = loss_batch.squeeze().tolist()\n","        else:\n","            loss_names = [f'{self.criterion_name}_loss_val', 'TC_loss_val', 'WT_loss_val', 'ET_loss_val']\n","            losses = loss_batch.squeeze().tolist()\n","        \n","        self.log_dict(\n","            dict(\n","                zip(\n","                    loss_names,\n","                    [loss.item(), *losses]\n","                   )\n","                ),\n","            batch_size = self.params.val_batch_size,\n","            on_step = False,\n","            on_epoch = True,\n","            rank_zero_only = True\n","            )\n","        \n","        \n","        ## Log the model predictions along with gt and its background image\n","        if batch_idx == 0:\n","            #batch_orig = [post_transform(i) for i in batch]\n","            \n","            input_t = batch['image'][0].as_tensor()[0,...] # For the `modality` log the FLAIR (0)\n","            output_t = batch['pred'][0].as_tensor()\n","            target_t = batch['label'][0].as_tensor()\n","            \n","            \n","            wandb_mask_list = log_brain_slices(input_t, output_t, target_t, total_slices=144)\n","            wandb.log({\"Segmentation Mask\" : wandb_mask_list}, commit=True)\n","        \n","        \n","        \n","        val_dsc = self.val_dice.compute()\n","        \n","        ## Eliminate \"bg val dice score\"\n","        if self.params.outClasses == 4:\n","            val_dsc = val_dsc[1:]\n","        \n","        ## Log val_scores\n","        self.log_dict(\n","            dict(\n","                zip(\n","                    ['dice_score', 'TC_dice_score', 'WT_dice_score', 'ET_dice_score'],\n","                    [val_dsc.mean().item(), val_dsc[0].item(), val_dsc[1].item(), val_dsc[2].item()]\n","                    )\n","                ),\n","            batch_size = self.params.val_batch_size,\n","            on_step = False,\n","            on_epoch = True,\n","            rank_zero_only = True\n","            )\n","        \n","        self.val_dice.reset()\n","    \n","        \n","    \n","    def configure_optimizers(self):\n","        if self.optimizer == 'sgd':\n","            optimizer = SGD(self.model.parameters(),\n","            lr = self.params.sgd['lr'],\n","            momentum = self.params.sgd['momentum'],\n","            weight_decay = self.params.sgd['weight_decay'])\n","        \n","        elif self.optimizer == 'adam':\n","            optimizer = Adam(self.model.parameters(),\n","            lr = self.params.adam['lr'],\n","            weight_decay = self.params.adam['weight_decay'])\n","        \n","        elif self.optimizer == 'rmsprop':\n","            optimizer = RMSprop(self.model.parameters(),\n","            lr = self.params.rmsprop['lr'],\n","            momentum = self.params.rmsprop['momentum'],\n","            alpha = self.params.rmsprop['alpha'],\n","            weight_decay = self.params.rmsprop['weight_decay'])\n","\n","        scheduler = CosineAnnealingLR(optimizer, T_max=self.params.max_epochs)\n","    \n","        return {\n","            \"optimizer\" : optimizer,\n","            \"lr_scheduler\" : {\n","                \"scheduler\" : scheduler,\n","                \"monitor\" : 'dice_val_loss',\n","                \"frequency\" : self.trainer.check_val_every_n_epoch\n","            }\n","        }"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Utils :toolbox:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Log: Volume & Masks :pen:"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.141455Z","iopub.status.busy":"2023-03-28T15:31:57.140309Z","iopub.status.idle":"2023-03-28T15:31:57.155034Z","shell.execute_reply":"2023-03-28T15:31:57.153946Z","shell.execute_reply.started":"2023-03-28T15:31:57.141392Z"},"trusted":true},"outputs":[],"source":["def labels():\n","    segmentation_ids = [0,1,2,3]\n","    segmentation_classes = [\"background\", \"Whole Tumor\", \"Tumor Core\", \"Enhancing Tumor\"]\n","    return dict(zip(segmentation_ids, segmentation_classes))\n","\n","\n","def wb_mask(bg_img, pred_mask, true_mask):\n","    \n","    return wandb.Image(bg_img, masks={\n","        'prediction' : {'mask_data' : pred_mask, 'class_labels' : labels()},\n","        'ground_truth' : {'mask_data' : true_mask, 'class_labels' : labels()}\n","    })\n","\n","def onehotToIndexes(labels):\n","    #ipdb.set_trace(context=6)\n","    labels = labels.bool()\n","    indexes = torch.zeros_like(labels[0, ...], dtype=torch.uint8)\n","\n","    # nb: To create a proper mask, this order must be respected\n","    indexes[labels[0, ...]] = 0 # background - CAN BE IGNORED\n","    indexes[labels[2, ...]] = 1 # Whole Tumor\n","    indexes[labels[1, ...]] = 2 # Tumor Core\n","    indexes[labels[3, ...]] = 3 # Enhancing Tumor\n","\n","    return indexes\n","\n","def log_brain_slices(volume, preds, gt, th: float=0.5, total_slices=120):\n","    \n","    wandb_mask_list = []\n","    \n","    volume = volume.detach().cpu()\n","    \n","    # Activate the predictions\n","    if preds.shape[0] == 4: # if there's also the background\n","        preds = preds[1:,...] # ... remove it\n","    \n","    preds = sigmoid(preds).detach().cpu()\n","    \n","    # filter for activated voxels exceeding an arbitrary threshold\n","    activated_voxels = where(preds>th, 1, 0)\n","    preds_idx = stack( [activated_voxels[1], activated_voxels[0]*2, activated_voxels[2]*3], dim=0) # WT=1, TC=2, ET=3\n","    preds_idx = amax(preds_idx, dim=0)\n","    \n","    # Remove the background class if present\n","    gt = gt.detach().cpu()\n","    \n","    if gt.shape[0] == 4:\n","        gt = gt[1:,...]\n","        \n","    gt_idx = amax(stack([gt[1], gt[0]*2, gt[2]*3], dim=0), dim=0).detach().cpu()\n","    \n","    # Transform to numpy arrays\n","    preds_idx = preds_idx.numpy()\n","    gt_idx = gt_idx.numpy()\n","    \n","    \n","    #gt_idx = onehotToIndexes(gt).detach().cpu().numpy()\n","    \n","    for vol_slice_idx in range(total_slices):\n","        #print(vol_slice_idx)\n","        img = volume[:, :, vol_slice_idx]\n","        prd = preds_idx[:, :, vol_slice_idx]\n","        gt = gt_idx[:, :, vol_slice_idx]\n","    \n","        wandb_mask_list.append(wb_mask(img, prd, gt))\n","    \n","    return wandb_mask_list"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Transforms :man_mechanic:"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.158842Z","iopub.status.busy":"2023-03-28T15:31:57.158515Z","iopub.status.idle":"2023-03-28T15:31:57.186312Z","shell.execute_reply":"2023-03-28T15:31:57.185258Z","shell.execute_reply.started":"2023-03-28T15:31:57.158814Z"},"trusted":true},"outputs":[],"source":["# @title Custom Transforms\n","\n","from monai.transforms import (\n","    MapTransform,\n","    InvertibleTransform,\n","    Compose,\n","    Invertd,\n","    LoadImaged,\n","    CropForegroundd,\n","    RandSpatialCropd,\n","    SpatialPadd,\n","    CenterSpatialCropd,\n","    EnsureChannelFirstd,\n","    EnsureTyped,\n","    Orientationd,\n","    Spacingd,\n","    SplitDimd,\n","    RandSpatialCropd,\n","    RandFlipd,\n","    NormalizeIntensity,\n","    NormalizeIntensityd,\n","    RandScaleIntensityd,\n","    RandShiftIntensityd,\n","    Activations,\n","    Activationsd,\n","    AsDiscrete,\n","    AsDiscreted,\n","    )\n","\n","class MyNormalizeIntensity(NormalizeIntensity, InvertibleTransform):\n","    \"\"\"\n","    Was originally created to make MONAI NormalizeIntensity invertible.\n","    The problem with their `transform` is that when the inversion is applied,\n","    it is performed on the whole set of (invertible) transformations applied originally and it seems\n","    that personalized keys cannot be targeted -> when appling this inversion, also the label was normalized (wrongly).\n","    \n","    I issued a suggestion to MONAI to make this transform invertible, and to date it has not been implemented.\n","    \"\"\"\n","    def __init__(self,\n","                 subtrahend: Union[Sequence, NdarrayOrTensor, None] = None,\n","                 divisor: Union[Sequence, NdarrayOrTensor, None] = None,\n","                 nonzero: bool = False,\n","                 channel_wise: bool = False,\n","                 dtype: DtypeLike = np.float32):\n","        super().__init__(subtrahend, divisor, nonzero, channel_wise, dtype)\n","    \n","    def _normalize(self, img: NdarrayOrTensor, sub=None, div=None) -> NdarrayOrTensor:\n","        \n","        img, *_ = convert_data_type(img, dtype=torch.float32)\n","\n","        if self.nonzero:\n","            slices = img != 0\n","        else:\n","            if isinstance(img, np.ndarray):\n","                slices = np.ones_like(img, dtype=bool)\n","            else:\n","                slices = torch.ones_like(img, dtype=torch.bool)\n","        if not slices.any():\n","            return img\n","\n","        _sub = sub if sub is not None else self._mean(img[slices])\n","        if isinstance(_sub, (torch.Tensor, np.ndarray)):\n","            _sub, *_ = convert_to_dst_type(_sub, img)\n","            _sub = _sub[slices]\n","\n","        _div = div if div is not None else self._std(img[slices])\n","        \n","        if np.isscalar(_div):\n","            if _div == 0.0:\n","                _div = 1.0\n","        elif isinstance(_div, (torch.Tensor, np.ndarray)):\n","            _div, *_ = convert_to_dst_type(_div, img)\n","            _div = _div[slices]\n","            _div[_div == 0.0] = 1.0\n","\n","        img[slices] = (img[slices] - _sub) / _div\n","        return img, [_sub, _div]\n","    \n","    def __call__(self, img: NdarrayOrTensor) -> NdarrayOrTensor:\n","        \"\"\"\n","        Apply the transform to `img`, assuming `img` is a channel-first array if `self.channel_wise` is True,\n","        \"\"\"\n","        \n","        img = convert_to_tensor(img, track_meta=get_track_meta())\n","        list_stats = []\n","        dtype = self.dtype or img.dtype\n","        if self.channel_wise:\n","            if self.subtrahend is not None and len(self.subtrahend) != len(img):\n","                raise ValueError(f\"img has {len(img)} channels, but subtrahend has {len(self.subtrahend)} components.\")\n","            if self.divisor is not None and len(self.divisor) != len(img):\n","                raise ValueError(f\"img has {len(img)} channels, but divisor has {len(self.divisor)} components.\")\n","\n","            #ipdb.set_trace(context=4)\n","            for i, d in enumerate(img):\n","                img[i], stats = self._normalize(d, sub=self.subtrahend[i] if self.subtrahend is not None else None, div=self.divisor[i] if self.divisor is not None else None)\n","                list_stats.append(stats)\n","            \n","        else:\n","            img = self._normalize(img, self.subtrahend, self.divisor)\n","\n","        out = convert_to_dst_type(img, img, dtype=dtype)[0]\n","    \n","        if get_track_meta():\n","            self.update_meta(tensor=out, stats=list_stats)\n","            self.push_transform(out, extra_info={'stats' : torch.tensor(list_stats)} )\n","        \n","        return out\n","  \n","    def update_meta(self, tensor: MetaTensor, stats):\n","        #ipdb.set_trace(context=4)\n","        tensor.mean = stats[0]\n","        tensor.std = stats[1]\n","        #self.inverse_update(tensor)\n","  \n","    def inverse(self, data: MetaTensor) -> MetaTensor:\n","        #ipdb.set_trace(context=4)\n","        transform = self.pop_transform(data)\n","        stats = transform[TraceKeys.EXTRA_INFO][\"stats\"]\n","        if stats.shape[0] != data.shape[0]:\n","            return data\n","        mean = stats[:,0].to(data.device) \n","        std = stats[:,1].to(data.device)\n","\n","        # perform the inversion\n","        out = data * std[..., None, None, None]\n","        out += mean[..., None, None, None]\n","\n","        return out\n","    \n","\n","class MyNormalizeIntensityd(NormalizeIntensityd, InvertibleTransform):\n","    \n","    def __init__(\n","        self,\n","        keys: KeysCollection,\n","        subtrahend: Optional[NdarrayOrTensor] = None,\n","        divisor: Optional[NdarrayOrTensor] = None,\n","        nonzero: bool = False,\n","        channel_wise: bool = False,\n","        dtype: DtypeLike = np.float32,\n","        allow_missing_keys: bool = False\n","    ):\n","        super().__init__(keys, allow_missing_keys)\n","        self.normalizer = MyNormalizeIntensity(subtrahend, divisor, nonzero, channel_wise, dtype)\n","\n","    \n","    def inverse(self, data: Mapping[Hashable, MetaTensor]) -> Dict[Hashable, MetaTensor]:    \n","        d = dict(data)\n","        for key in self.key_iterator(d):\n","            d[key] = self.normalizer.inverse(d[key])\n","        return d\n","\n","\n","\n","class ToMultiChannelBratsClassesd(MapTransform):\n","    \"\"\"\n","    Convert labels to multi channels based on brats classes\n","    label 1 is the peritumoral edema\n","    label 2 is the GD-enhancing tumor\n","    label 3 is the necrotic and non-enhancing tumor core\n","\n","    The classes are: TC (Tumore core), WT (Whole tumor), ET (Enhancing tumor)\n","    \"\"\"\n","\n","    def __call__(self, data):\n","        d = dict(data)\n","        for key in self.keys:\n","            ## label 0 id the background\n","            ## merge label 2 and 3 to consrtuct TC\n","            ## merge label 1, 2 and 3 to construct WT\n","            ## label 2 is ET\n","            result = [(d[key]==2) | (d[key]==3), (d[key]==1) | (d[key]==2) | (d[key]==3), d[key]==2 ]\n","            d[key] = stack(result, axis=0).float()\n","        return d"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train/Val/Test Transforms"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.190622Z","iopub.status.busy":"2023-03-28T15:31:57.190042Z","iopub.status.idle":"2023-03-28T15:31:57.245874Z","shell.execute_reply":"2023-03-28T15:31:57.244717Z","shell.execute_reply.started":"2023-03-28T15:31:57.190593Z"},"trusted":true},"outputs":[],"source":["# @title Data Transforms\n","roi_size = (128, 128, 128) # (192, 192, 144)\n","\n","# Define data transformation\n","train_transform = Compose(\n","  [\n","      # Load 4 Nifti images and stack them together (4 in_modalities)\n","      LoadImaged(keys=[\"image\", \"label\"]),\n","      EnsureChannelFirstd(keys=\"image\"),\n","      # here was SplitDimd\n","      EnsureTyped(keys=['image', 'label']),\n","      ToMultiChannelBratsClassesd(keys='label'),\n","      Orientationd(keys=['image', 'label'], axcodes=\"RAS\"), # Right-Anterior-Superior\n","      Spacingd(\n","          keys=['image', 'label'],\n","          pixdim=(1.0, 1.0, 1.0), # Resample input image into the specified pixdim\n","          mode=('bilinear', 'nearest')\n","      ),\n","      CropForegroundd(\n","          keys=[\"image\", \"label\"],\n","          source_key = \"image\",\n","          channel_indices = 0, # use FLAIR to compute the b_box\n","          #k_divisible = [roi_size[0], roi_size[1], roi_size[2]] # comment since afterward CenterSpatialCrop cuts the image to roi_size\n","                     ),\n","      CenterSpatialCropd(\n","          keys=['image', 'label'],\n","          roi_size=[roi_size[0], roi_size[1], roi_size[2]]\n","      ),\n","      SpatialPadd(\n","          keys = ['image', 'label'],\n","          spatial_size = [roi_size[0], roi_size[1], roi_size[2]]\n","      ),\n","      RandFlipd(keys=['image', 'label'], prob=0.5, spatial_axis=0),\n","      RandFlipd(keys=['image', 'label'], prob=0.5, spatial_axis=1),\n","      RandFlipd(keys=['image', 'label'], prob=0.5, spatial_axis=2),\n","      NormalizeIntensityd(keys='image', nonzero=True, channel_wise=True), # If subtrahend and divisor are None -> use mean and std of the image/image channel (if `channek_wise` = True)\n","      RandScaleIntensityd(keys='image', factors=0.1, prob=1.0), # if `factors` is single number 'f' -> [-f; f]\n","      RandShiftIntensityd(keys='image', offsets=0.1, prob=1.0),\n","      \n","      ## NOTE: SplitDimd creates copies of the original data.\n","      ## splitting 4 multichannel images result into: original 4 MC image + 4 single channel (SC) images.\n","      ## To date, it seems impossible to reduce memory consumption by loading SC images from Decathlon Dataset.\n","      ## The only viable option is to work with the more recent BraTS 2021 dataset,\n","      ##  which comes divided per patient, but the modalities are stored in separated files.\n","       \n","      #SplitDimd(dim = 0, keys=['image'], list_output=False), # Use it for single inModality training\n","  ]  \n",")\n","\n","val_transform = Compose(\n","  [\n","      LoadImaged(keys=['image', 'label']),\n","      EnsureChannelFirstd(keys='image'),\n","      # here was SplitDimd\n","      EnsureTyped(keys=['image', 'label']),\n","      ToMultiChannelBratsClassesd(keys='label'),\n","      Orientationd(keys=['image', 'label'], axcodes='RAS'),\n","      Spacingd(\n","          keys=['image', 'label'],\n","          pixdim=(1.0, 1.0, 1.0),\n","          mode=('bilinear', 'nearest')\n","      ),\n","      #RandSpatialCropd(keys=['image', 'label'], roi_size=[224, 224, 144], random_size=False), # Make if False to have specifically the requested `roi_size`\n","      NormalizeIntensityd(keys='image', nonzero=True, channel_wise=True),\n","      #SplitDimd(dim = 0, keys=['image'], list_output=False),\n","  ]\n",")\n","\n","val_act_threshold = Compose(\n","    [\n","        Activations(sigmoid=True),\n","        AsDiscrete(threshold=0.5)\n","    ])\n","\n","val_log_slice_transform = Compose(\n","    [\n","        Activations(sigmoid=True)\n","    ]\n",")\n","\n","\n","\n","post_transform = Compose([\n","    Invertd(\n","        keys=['image', 'label'],\n","        transform = val_transform,\n","        orig_keys= ['image', 'label'],\n","        meta_keys = ['image_meta_dict', 'label_meta_dict'],\n","        orig_meta_keys = ['image_meta_dict', 'label_meta_dict'],\n","        meta_key_postfix = 'meta_dict',\n","        nearest_interp = False,\n","        to_tensor = True,\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        allow_missing_keys=False\n","    )\n","])\n","\n","post_transform_pred = Compose([\n","    Invertd(\n","        keys=\"pred\", # w only \"pred\" works fine\n","        transform=val_transform,\n","        orig_keys=\"image\",\n","        #meta_keys=\"pred_meta_dict\",\n","        orig_meta_keys=\"image_meta_dict\",\n","        meta_key_postfix=\"meta_dict\",\n","        nearest_interp=False,\n","        to_tensor=False,\n","        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n","        allow_missing_keys=False\n","    ),\n","    #Activationsd(keys=\"pred\", sigmoid=True),\n","    #AsDiscreted(keys=\"pred\", threshold=0.5),\n","])\n","\n","\n","transform = {'transform' : \n","              {\n","                  'training' : train_transform,\n","                  'validation' : val_transform\n","               }\n","             }"]},{"cell_type":"markdown","metadata":{},"source":["\n","## Models"]},{"cell_type":"markdown","metadata":{},"source":["#### Unet Submodules"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.323036Z","iopub.status.busy":"2023-03-28T15:31:57.322456Z","iopub.status.idle":"2023-03-28T15:31:57.345796Z","shell.execute_reply":"2023-03-28T15:31:57.344403Z","shell.execute_reply.started":"2023-03-28T15:31:57.323007Z"},"trusted":true},"outputs":[],"source":["class UnetConv3(nn.Module):\n","    def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1), dropout_rate=0.2):\n","        super(UnetConv3, self).__init__()\n","\n","        if is_batchnorm:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv3d(in_size, out_size, kernel_size=kernel_size, stride=init_stride, padding=padding_size, bias=False),\n","                nn.BatchNorm3d(out_size),\n","                nn.ReLU6(inplace=True),\n","                nn.Dropout3d(p=dropout_rate)\n","            )\n","            \n","            self.conv2 = nn.Sequential(\n","                nn.Conv3d(out_size, out_size, kernel_size=kernel_size, stride=1, padding=padding_size),\n","                nn.BatchNorm3d(out_size),\n","                nn.ReLU6(inplace=True),\n","                nn.Dropout3d(p=dropout_rate)\n","            )\n","        else:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv3d(in_size, out_size, kernel_size=kernel_size, stride=init_stride, padding=padding_size),                       \n","                nn.ReLU6(inplace=True),\n","                nn.Dropout3d(p=dropout_rate)\n","                )\n","            \n","            self.conv2 = nn.Sequential(\n","                nn.Conv3d(out_size, out_size, kernel_size=kernel_size, stride=1, padding=padding_size),\n","                nn.ReLU6(inplace=True),\n","                nn.Dropout3d(p=dropout_rate)\n","            )\n","        \n","        # initialise the blocks\n","        for m in self.children():\n","            init_weights(m, init_type='kaiming')\n","    \n","\n","    def forward(self, inputs):\n","        outputs = self.conv1(inputs)\n","        outputs = self.conv2(outputs)\n","        return outputs\n","\n","\n","class UnetGridGatingSignal3(nn.Module):\n","    def __init__(self, in_size, out_size, kernel_size=(1,1,1), is_batchnorm=True):\n","        super(UnetGridGatingSignal3, self).__init__()\n","\n","        if is_batchnorm:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0), bias=False),\n","                nn.BatchNorm3d(out_size),\n","                nn.ReLU6(inplace=True),\n","                )\n","        \n","        else:\n","            self.conv1 = nn.Sequential(\n","                nn.Conv3d(in_size, out_size, kernel_size, (1,1,1), (0,0,0)),\n","                nn.ReLU6(inplace=True),\n","                )\n","        \n","        for m in self.children():\n","            init_weights(m, init_type=\"kaiming\")\n","        \n","    def forward(self, inputs):\n","        outputs = self.conv1(inputs)\n","        return outputs\n","\n","\n","class UnetUp3(nn.Module):\n","    def __init__(self, in_size, out_size, is_deconv, is_batchnorm=True):\n","        super(UnetUp3, self).__init__()\n","        if is_deconv:\n","            self.conv = UnetConv3(in_size, out_size, is_batchnorm)\n","            self.up = nn.ConvTranspose3d(in_size, out_size, kernel_size=(4,4,1), stride=(2,2,1), padding=(1,1,0))\n","        else:\n","            self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm)\n","            self.up = nn.Upsample(scale_factor=(2,2,1), mode='trilinear')\n","        \n","        # initialise the blocks\n","        for m in self.children():\n","            if m.__class__.__name__.find('UnetConv3') != -1: continue\n","            init_weights(m, init_type='kaiming')\n","    \n","    def forward(self, inputs1, inputs2):\n","        outputs2 = self.up(inputs2)\n","        offset = outputs2.size()[2] - inputs1.size()[2] # what's this 'offset'?\n","        padding = 2 * [offset // 2, offset // 2, 0]\n","        outputs1 = F.pad(inputs1, padding)\n","        return self.conv(cat([outputs1, outputs2], 1))\n","\n","class UnetUp3_CT(nn.Module):\n","    def __init__(self, in_size, out_size, is_batchnorm=True):\n","        super(UnetUp3_CT, self).__init__()\n","        \n","        self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.up = nn.Upsample(scale_factor=(2,2,2), mode='trilinear')\n","        \n","        # initialise the blocks\n","        for m in self.children():\n","            if m.__class__.__name__.find('UnetConv3') != -1: continue\n","            init_weights(m, init_type='kaiming')\n","    \n","    def forward(self, inputs1, inputs2):\n","        outputs2 = self.up(inputs2)\n","        offset = outputs2.size()[2] - inputs1.size()[2] # what's this 'offset'?\n","        padding = 2 * [offset // 2, offset // 2, 0]\n","        outputs1 = F.pad(inputs1, padding)\n","        return self.conv(cat([outputs1, outputs2], 1))"]},{"cell_type":"markdown","metadata":{},"source":["#### Attention Submodules"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.348493Z","iopub.status.busy":"2023-03-28T15:31:57.347621Z","iopub.status.idle":"2023-03-28T15:31:57.372935Z","shell.execute_reply":"2023-03-28T15:31:57.371827Z","shell.execute_reply.started":"2023-03-28T15:31:57.348453Z"},"trusted":true},"outputs":[],"source":["class _GridAttentionBlockND(nn.Module):\n","    def __init__(self, in_channels, gating_channels, inter_channels=None, dimension=3, mode='concatenation',\n","                 sub_sample_factor=(2,2,2)):\n","        super(_GridAttentionBlockND, self).__init__()\n","\n","        assert dimension in [2,3]\n","        assert mode in ['concatenation', 'concatenation_debug', 'concatenation_residual']\n","\n","        # Downsampling rate for the input featuremap\n","        if isinstance(sub_sample_factor, tuple): self.sub_sample_factor = sub_sample_factor\n","        elif isinstance(sub_sample_factor, list): self.sub_sample_factor = tuple(sub_sample_factor)\n","        else: self.sub_sample_factor = tuple([sub_sample_factor]) * dimension\n","\n","        # Default parameter set\n","        self.mode = mode\n","        self.dimension = dimension\n","        self.sub_sample_kernel_size = self.sub_sample_factor\n","\n","        # Number of channels (pixel dimensions)\n","        self.in_channels = in_channels\n","        self.gating_channels = gating_channels\n","        self.inter_channels = inter_channels\n","\n","        if self.inter_channels is None:\n","            self.inter_channels = in_channels // 2\n","            if self.inter_channels == 0:\n","                self.inter_channels = 1\n","        \n","        if dimension == 3:\n","            conv_nd = nn.Conv3d\n","            bn = nn.BatchNorm3d\n","            self.upsample_mode = 'trilinear'\n","        elif dimension == 2:\n","            conv_nd = nn.Conv2d\n","            bn = nn.BatchNorm2d\n","            self.upsample_mode = 'bilinear'\n","        else:\n","            raise NotImplementedError\n","\n","        # Output transform\n","        self.W = nn.Sequential(\n","            conv_nd(in_channels = self.in_channels, out_channels= self.in_channels, kernel_size=1, stride=1, padding=0),\n","            bn(self.in_channels),\n","        )\n","\n","        # Theta^T * x_ij + Phi^T * gating_signal + bias\n","        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n","                             kernel_size=self.sub_sample_kernel_size, stride=self.sub_sample_factor, padding=0, bias=False)\n","        \n","        self.phi = conv_nd(in_channels=self.gating_channels, out_channels=self.inter_channels,\n","                           kernel_size=1, stride=1, padding=0, bias=True)\n","        \n","        self.psi = conv_nd(in_channels=self.inter_channels, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n","\n","\n","        # Initialise weights\n","        for m in self.children():\n","            init_weights(m, init_type='kaiming')\n","        \n","        # Define the operation\n","        if mode == 'concatenation':\n","            self.operation_function = self._concatenation\n","        elif mode == 'concatenation_debug':\n","            self.operation_function = self._concatenation_debug\n","        elif mode == 'concatenation_residual':\n","            self.operation_function = self._concatenation_residual\n","        else:\n","            raise NotImplementedError('Unknown operation function.')\n","    \n","    def forward(self, x, g):\n","        '''\n","         :param x: (b, c, t, h, w)\n","         :param g: (b, g_d)\n","         :return:\n","        '''\n","\n","        output = self.operation_function(x, g)\n","        return output\n","    \n","\n","    def _concatenation(self, x, g):\n","        input_size = x.size()\n","        batch_size = input_size[0]\n","        assert batch_size == g.size(0)\n","\n","        # theta -> (b, c, t, h, w) -> (b, i_c, t, h, w) -> (b, i_c, thw)\n","        # phi -> (b, g_d) -> (b, i_c)\n","        theta_x = self.theta(x)\n","        theta_x_size = theta_x.size()\n","\n","        # g (b, c, t', h', w') -> phi_g (b, i_c, t', h', w')\n","        # Relu(theta_x + phi_g + bias) -> f = (b, i_c, thw) -> (b, i_c. t/s1, h/s2, w/s3)\n","        phi_g = F.interpolate(self.phi(g), size=theta_x_size[2:], mode=self.upsample_mode)\n","        f = F.relu6(theta_x + phi_g, inplace=True)\n","\n","        # psi^T * f -> (b, psi_i_c, t/s1, h/s2, w/s3)\n","        sigm_psi_f = sigmoid(self.psi(f))\n","\n","        # upsample the attention and multiply\n","        sigm_psi_f = F.interpolate(sigm_psi_f, size=input_size[2:], mode=self.upsample_mode)\n","        y = sigm_psi_f.expand_as(x) * x\n","        W_y = self.W(y) # check why this conv is performed, and how it does transform the 'element-wise' multiplication between x' and g'\n","\n","        return W_y, sigm_psi_f\n","\n","\n","\n","\n","\n","\n","\n","class GridAttentionBlock3D(_GridAttentionBlockND):\n","    def __init__(self, in_channels, gating_channels, inter_channels=None, mode='concatenation',\n","                 sub_sample_factor=(2,2,2)):\n","        super(GridAttentionBlock3D, self).__init__(in_channels,\n","                                                   gating_channels = gating_channels,\n","                                                   inter_channels=inter_channels,\n","                                                   dimension = 3,\n","                                                   mode = mode,\n","                                                   sub_sample_factor = sub_sample_factor,\n","                                                   )\n","\n","\n","class MultiAttentionBlock(nn.Module):\n","    def __init__(self, in_size, gate_size, inter_size, nonlocal_mode, sub_sample_factor):\n","        super(MultiAttentionBlock, self).__init__()\n","        self.gate_block_1 = GridAttentionBlock3D(in_channels=in_size, gating_channels=gate_size,\n","                                                 inter_channels=inter_size, mode=nonlocal_mode,\n","                                                 sub_sample_factor=sub_sample_factor)\n","        self.gate_block_2 = GridAttentionBlock3D(in_channels=in_size, gating_channels=gate_size,\n","                                                 inter_channels=inter_size, mode=nonlocal_mode,\n","                                                 sub_sample_factor=sub_sample_factor)\n","        self.combine_gates = nn.Sequential(nn.Conv3d(in_size*2, in_size, kernel_size=1, stride=1, padding=0), # *2 since we will concatenate the fmaps coming from 'g' and 'x'\n","                                           nn.BatchNorm3d(in_size),\n","                                           nn.ReLU6(inplace=True)\n","                                           )\n","    \n","        # initialise the blocks\n","        for m in self.children():\n","            if m.__class__.__name__.find('GridAttentionBlock3D') != -1: continue\n","            init_weights(m, init_type='kaiming')\n","    \n","    def forward(self, input, gating_signal):\n","        gate_1, attention_1 = self.gate_block_1(input, gating_signal)\n","        gate_2, attention_2 = self.gate_block_2(input, gating_signal)\n","\n","        return self.combine_gates(cat([gate_1, gate_2], 1)), cat([attention_1, attention_2], 1)"]},{"cell_type":"markdown","metadata":{},"source":["#### DeepSupervision Submodules"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.375026Z","iopub.status.busy":"2023-03-28T15:31:57.374642Z","iopub.status.idle":"2023-03-28T15:31:57.386420Z","shell.execute_reply":"2023-03-28T15:31:57.385223Z","shell.execute_reply.started":"2023-03-28T15:31:57.374989Z"},"trusted":true},"outputs":[],"source":["class UnetDsv3(nn.Module):\n","    def __init__(self, in_size, out_size, scale_factor):\n","        super(UnetDsv3, self).__init__()\n","        self.dsv = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size=1, stride=1, padding=0),\n","                                 nn.Upsample(scale_factor=scale_factor, mode='trilinear'))\n","    \n","    def forward(self, input):\n","        return self.dsv(input)"]},{"cell_type":"markdown","metadata":{},"source":["#### utils"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.389582Z","iopub.status.busy":"2023-03-28T15:31:57.388180Z","iopub.status.idle":"2023-03-28T15:31:57.415102Z","shell.execute_reply":"2023-03-28T15:31:57.414135Z","shell.execute_reply.started":"2023-03-28T15:31:57.389550Z"},"trusted":true},"outputs":[],"source":["def init_weights(net, init_type='normal'):\n","    if init_type == 'kaiming':\n","        net.apply(weights_init_kaiming)\n","    else:\n","        raise NotImplementedError('Initialization method [%s] is not implemented' % init_type)\n","\n","def weights_init_kaiming(m):\n","    classname = m.__class__.__name__\n","    #print(classname)\n","    if classname.find('Conv') != -1:\n","        kaiming_normal_(m.weight, a=0.2, mode='fan_in', nonlinearity='relu' )\n","    elif classname.find('Linear') != -1:\n","        kaiming_normal_(m.weight, a=0.2, mode='fan_in', nonlinearity='relu' )\n","    elif classname.find('BatchNorm') != -1:\n","        normal_(m.weight, mean=1.0, std=0.02)\n","        constant_(m.bias, 0.0)\n","\n","def printInfo(model):\n","    C, H, W, D = 4, 128, 128, 128\n","    image_size = ((C, H, W, D))\n","    batch_size = (1,)\n","    input_size = batch_size + image_size\n","    device = \"cuda:0\" if is_available() else \"cpu\"\n","\n","\n","    # gets printed by default\n","    summary = torchinfo.summary(\n","        model,\n","        device = device,\n","        input_size = input_size,\n","        mode = \"train\",\n","        col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"),\n","        verbose = 1,\n","        depth=1\n","        )\n","    \n","    \n","    model = model.to(device)\n","    input_tensor = rand(input_size, device=device)\n","\n","    out = model(input_tensor)\n","\n","    print(out.shape)\n","\n","\n","alphas = [0.2391, 0.6477, 0.1132] # alpha values for WT, TC, ET (rarest class)\n","#alphas = [1, 1, 0.1747] #  # alpha values based on the original target (EDEMA, ET, NECROTIC) -> downweight only the rarest label  \n","\n","def CreateLoss(params: dict={}):\n","    \n","    criterion_name = params['criterion_name'] \n","    \n","    if criterion_name == \"dice\":\n","        include_background = params['n_classes'] == 3 # if n_classes == 4 -> there also the 'bg' class and it must be excluded\n","        squared_pred = params['squared_pred']\n","        to_onehot_y = params['to_onehot_y']\n","        sigmoid = params['sigmoid']\n","        reduction = params['dice_reduction']\n","        smooth_nr = params['smooth_nr']\n","        smooth_dr = params['smooth_dr']\n","        \n","        \n","        return DiceLoss(include_background=include_background,\n","                        squared_pred=squared_pred,\n","                        to_onehot_y=to_onehot_y,\n","                        sigmoid=sigmoid,\n","                        reduction=reduction,\n","                        smooth_nr=smooth_nr,\n","                        smooth_dr=smooth_dr\n","                       )\n","    elif criterion_name == \"focal\":\n","        include_background = params['n_classes'] == 3 # if n_classes == 4 -> there also the 'bg' class and it must be excluded\n","        to_onehot_y = params['to_onehot_y']\n","        gamma = params['gamma']\n","        reduction = params['dice_reduction']\n","        \n","        return FocalLoss(include_background=include_background,\n","                         to_onehot_y = to_onehot_y,\n","                         gamma = gamma,\n","                         weight = alphas,\n","                         reduction = reduction)\n","\n","    \n","        \n","    elif criterion_name == \"dicefocal\":\n","        \n","        lambda_dice = params['lambda_dice']\n","        lambda_focal = params['lambda_focal']\n","        \n","        dice_params = dict(params)\n","        focal_params = dict(params)\n","        dice_params.update({'criterion_name':'dice'})\n","        focal_params.update({'criterion_name':'focal'})\n","        \n","        \n","        return {\"dice\": CreateLoss(dice_params),\n","                 \"focal\":CreateLoss(focal_params),\n","                 \"lambda_dice\": lambda_dice,\n","                 \"lambda_focal\": lambda_focal}\n","\n","\n","def createModel(params: dict={}):\n","    \n","    model_name = params[\"model_name\"].lower()\n","    \n","    \n","    if model_name == \"segresnet\":\n","        blocks_down = params['blocks_down'] #n down_sample blocks in each layer\n","        blocks_up = params['blocks_up'] # n up_sample blocks in each layer\n","        init_filters = params['init_filters'] # n out_channels for initial convolution layer\n","        in_channels = params['in_channels']\n","        out_channels = params['out_channels'] # 4 = bg + relevant classes\n","        dropout_prob = params['dropout_prob']\n","        upsample_mode = params['upsample_mode'] # 'nontrainable' -> non trainable linear interp. (torch.nnUpsample).\n","                                           # 'deconv' uses trainable tranpose conv layers\n","        \n","        return SegResNet(\n","            blocks_down = blocks_down,\n","            blocks_up = blocks_up,\n","            init_filters = init_filters, \n","            in_channels = in_channels,\n","            out_channels = out_channels, \n","            dropout_prob = dropout_prob,\n","            upsample_mode = upsample_mode \n","        )\n","    \n","    \n","    elif model_name == \"segresnetvae\":\n","        \n","        input_image_size = params['input_image_size']\n","        vae_estimate_std = params['vae_estimate_std']\n","        vae_default_std = params['vae_default_std']\n","        vae_nz = params['vae_nz']\n","        blocks_down = params['blocks_down'] #n down_sample blocks in each layer\n","        blocks_up = params['blocks_up'] # n up_sample blocks in each layer\n","        init_filters = params['init_filters'] # n out_channels for initial convolution layer\n","        in_channels = params['in_channels']\n","        out_channels = params['out_channels'] # 4 = bg + relevant classes\n","        dropout_prob = params['dropout_prob']\n","        upsample_mode = params['upsample_mode'] # 'nontrainable' -> non trainable linear interp. (torch.nnUpsample).\n","                                           # 'deconv' uses trainable tranpose conv layers\n","        \n","        \n","        \n","        return MySegResNetVAE(\n","            input_image_size = input_image_size,\n","            vae_estimate_std = vae_estimate_std,\n","            vae_default_std = vae_default_std,\n","            vae_nz = vae_nz,\n","            init_filters = init_filters,\n","            blocks_down = blocks_down,\n","            blocks_up = blocks_up,\n","            in_channels = in_channels,\n","            out_channels = out_channels,\n","            dropout_prob = dropout_prob,\n","            upsample_mode = upsample_mode\n","        )\n","    \n","    elif model_name == \"attentionunet\":\n","        feature_scale = params['feature_scale']\n","        in_channels = params['in_channels']\n","        out_channels = params['out_channels'] \n","        is_deconv = params['is_deconv']\n","        nonlocal_mode = params['nonlocal_mode']\n","        attention_dsample = tuple(params['attention_dsample'])\n","        is_batchnorm = params['is_batchnorm']\n","        dropout_prob = params['dropout_prob']\n","        \n","        return MultiAttentionUnet(\n","            feature_scale = feature_scale,\n","            in_channels = in_channels,\n","            n_classes = out_channels,\n","            nonlocal_mode = nonlocal_mode,\n","            attention_dsample = attention_dsample,\n","            is_batchnorm = is_batchnorm,\n","            is_deconv = is_deconv,\n","            dropout_prob = dropout_prob)"]},{"cell_type":"markdown","metadata":{},"source":["## Attention Unet"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.417518Z","iopub.status.busy":"2023-03-28T15:31:57.416839Z","iopub.status.idle":"2023-03-28T15:31:57.443128Z","shell.execute_reply":"2023-03-28T15:31:57.442026Z","shell.execute_reply.started":"2023-03-28T15:31:57.417411Z"},"trusted":true},"outputs":[],"source":["class MultiAttentionUnet(nn.Module):\n","\n","    def __init__(self, feature_scale=4, n_classes=3, is_deconv=True, in_channels=4,\n","                 nonlocal_mode='concatenation', attention_dsample=(2,2,2), is_batchnorm=True, dropout_prob=0.2):\n","        super(MultiAttentionUnet, self).__init__()\n","        self.is_deconv = is_deconv # might be unused\n","        self.in_channels = in_channels\n","        self.is_batchnorm = is_batchnorm\n","        self.feature_scale = feature_scale\n","\n","\n","        filters = [64, 128, 256, 512, 1024] # [32, 64, 128, 512, 1024] #\n","        filters = [int(x / self.feature_scale) for x in filters]\n","        \n","        # dropout - added for reducing overfit and regulariing loss descent\n","        self.dropout = nn.Dropout3d(p=dropout_prob)\n","\n","        # downsampling\n","        self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.maxpool1 = nn.MaxPool3d(kernel_size=(2,2,2))\n","        \n","        self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.maxpool2 = nn.MaxPool3d(kernel_size=(2,2,2))\n","        \n","        self.conv3 = UnetConv3(filters[1], filters[2], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.maxpool3 = nn.MaxPool3d(kernel_size=(2,2,2))\n","        \n","        self.conv4 = UnetConv3(filters[2], filters[3], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.maxpool4 = nn.MaxPool3d(kernel_size=(2,2,2))\n","\n","        self.center = UnetConv3(filters[3], filters[4], self.is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n","        self.gating = UnetGridGatingSignal3(filters[4], filters[4], kernel_size=(1,1,1), is_batchnorm=self.is_batchnorm)\n","\n","        # attention blocks\n","\n","\n","        self.attentionblock2 = MultiAttentionBlock(in_size=filters[1], gate_size=filters[2], inter_size=filters[1],\n","                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n","        self.attentionblock3 = MultiAttentionBlock(in_size=filters[2], gate_size=filters[3], inter_size=filters[2],\n","                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n","        self.attentionblock4 = MultiAttentionBlock(in_size=filters[3], gate_size=filters[4], inter_size=filters[3],\n","                                                   nonlocal_mode=nonlocal_mode, sub_sample_factor= attention_dsample)\n","        \n","        # upsampling\n","        self.up_concat4 = UnetUp3_CT(filters[4], filters[3], is_batchnorm)\n","        self.up_concat3 = UnetUp3_CT(filters[3], filters[2], is_batchnorm)\n","        self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)\n","        self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)\n","\n","        # deep supervision\n","        self.dsv4 = UnetDsv3(in_size=filters[3],  out_size=n_classes, scale_factor=8)\n","        self.dsv3 = UnetDsv3(in_size=filters[2],  out_size=n_classes, scale_factor=4)\n","        self.dsv2 = UnetDsv3(in_size=filters[1],  out_size=n_classes, scale_factor=2)\n","        self.dsv1 = nn.Conv3d(in_channels=filters[0],  out_channels=n_classes, kernel_size=1)\n","\n","        # final conv (without concat)\n","        self.final = nn.Conv3d(n_classes*4, n_classes, 1)\n","\n","        # initialise weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv3d):\n","                init_weights(m, init_type='kaiming')\n","            elif isinstance(m, nn.BatchNorm3d):\n","                init_weights(m, init_type='kaiming')\n","    \n","    def forward(self, inputs):\n","        # Feature Extraction\n","        conv1 = self.conv1(inputs)\n","        maxpool1 = self.maxpool1(conv1)\n","        maxpool1 = self.dropout(maxpool1)\n","        \n","        conv2 = self.conv2(maxpool1)\n","        maxpool2 = self.maxpool2(conv2)\n","        maxpool2 = self.dropout(maxpool2)\n","        \n","        conv3 = self.conv3(maxpool2)\n","        maxpool3 = self.maxpool3(conv3)\n","        maxpool3 = self.dropout(maxpool3)\n","        \n","        conv4 = self.conv4(maxpool3)\n","        maxpool4 = self.maxpool4(conv4)\n","        maxpool4 = self.dropout(maxpool4)\n","\n","        # Gating Signal Generation\n","        center = self.center(maxpool4)\n","        center = self.dropout(center)\n","        gating = self.gating(center)\n","\n","        # Attention Mechanism\n","        # Upscaling Part (Decoder)\n","        g_conv4, att4 = self.attentionblock4(conv4, gating)\n","        up4 = self.up_concat4(g_conv4, center)\n","\n","        g_conv3, att3 = self.attentionblock3(conv3, up4)\n","        up3 = self.up_concat3(g_conv3, up4)\n","        \n","        g_conv2, att2 = self.attentionblock2(conv2, up3)\n","        up2 = self.up_concat2(g_conv2, up3)\n","        up1 = self.up_concat1(conv1, up2)\n","\n","        # Deep Supervision\n","        if self.training:\n","            #dsv4 = self.dsv4(up4) # Convolve and Upsample\n","            dsv3 = self.dsv3(up3)\n","            dsv2 = self.dsv2(up2)\n","            dsv1 = self.dsv1(up1)\n","            #final = self.final(cat([dsv1, dsv2, dsv3, dsv4], dim=1))\n","            final = stack([dsv1, dsv2, dsv3], dim=1) # stack them from \"up tp bottom\"\n","        else:\n","            final = self.dsv1(up1) # Apply only the final convolution\n","        \n","\n","        return final"]},{"cell_type":"markdown","metadata":{},"source":["## MySegResNetVAE"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.445500Z","iopub.status.busy":"2023-03-28T15:31:57.445073Z","iopub.status.idle":"2023-03-28T15:31:57.464523Z","shell.execute_reply":"2023-03-28T15:31:57.463385Z","shell.execute_reply.started":"2023-03-28T15:31:57.445412Z"},"trusted":true},"outputs":[],"source":["class MySegResNetVAE(SegResNetVAE):\n","    def __init__(\n","        self,\n","        input_image_size: Sequence[int],\n","        vae_estimate_std: bool = False,\n","        vae_default_std: float = 0.3,\n","        vae_nz: int = 256,\n","        spatial_dims: int = 3,\n","        init_filters: int = 8,\n","        in_channels: int = 1,\n","        out_channels: int = 2,\n","        dropout_prob: Optional[float] = None,\n","        act: Union[str, tuple] = (\"RELU\", {\"inplace\": True}),\n","        norm: Union[Tuple, str] = (\"GROUP\", {\"num_groups\": 8}),\n","        use_conv_final: bool = True,\n","        blocks_down: tuple = (1, 2, 2, 4),\n","        blocks_up: tuple = (1, 1, 1),\n","        upsample_mode: Union[UpsampleMode, str] = UpsampleMode.NONTRAINABLE,\n","    ):\n","        super().__init__(\n","            input_image_size = input_image_size,\n","            spatial_dims=spatial_dims,\n","            init_filters=init_filters,\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            dropout_prob=dropout_prob,\n","            act=act,\n","            norm=norm,\n","            use_conv_final=use_conv_final,\n","            blocks_down=blocks_down,\n","            blocks_up=blocks_up,\n","            upsample_mode=upsample_mode,\n","        )\n","\n","        self.input_image_size = input_image_size\n","        self.smallest_filters = 16\n","\n","        zoom = 2 ** (len(self.blocks_down) - 1)\n","        self.fc_insize = [s // (2 * zoom) for s in self.input_image_size]\n","\n","        self.vae_estimate_std = vae_estimate_std\n","        self.vae_default_std = vae_default_std\n","        self.vae_nz = vae_nz\n","        self._prepare_vae_modules()\n","        self.vae_conv_final = self._make_final_conv(in_channels)\n","    \n","    def _get_vae_loss(self, net_input: torch.Tensor, vae_input: torch.Tensor):\n","        \"\"\"\n","        Args:\n","        net_input: the original input of the network.\n","        vae_input: the input of VAE module, which is also the output of the network's encoder.\n","        \"\"\"\n","        x_vae = self.vae_down(vae_input)\n","        x_vae = x_vae.view(-1, self.vae_fc1.in_features)\n","        z_mean = self.vae_fc1(x_vae)\n","\n","        z_mean_rand = torch.randn_like(z_mean)\n","        z_mean_rand.requires_grad_(False)\n","\n","        if self.vae_estimate_std:\n","            z_sigma = self.vae_fc2(x_vae)\n","            z_sigma = F.softplus(z_sigma)\n","            vae_reg_loss = 0.5 * torch.mean(z_mean**2 + z_sigma**2 - torch.log(1e-8 + z_sigma**2) - 1)\n","\n","            x_vae = z_mean + z_sigma * z_mean_rand\n","        else:\n","            z_sigma = self.vae_default_std\n","            vae_reg_loss = torch.mean(z_mean**2)\n","\n","            x_vae = z_mean + z_sigma * z_mean_rand\n","\n","        x_vae = self.vae_fc3(x_vae)\n","        x_vae = self.act_mod(x_vae)\n","        x_vae = x_vae.view([-1, self.smallest_filters] + self.fc_insize)\n","        x_vae = self.vae_fc_up_sample(x_vae)\n","\n","        for up, upl in zip(self.up_samples, self.up_layers):\n","            x_vae = up(x_vae)\n","            x_vae = upl(x_vae)\n","\n","        x_vae = self.vae_conv_final(x_vae)\n","        vae_mse_loss = F.mse_loss(net_input, x_vae)\n","        vae_loss = vae_reg_loss + vae_mse_loss\n","        return vae_reg_loss, vae_mse_loss\n","    \n","    def forward(self, x):\n","        net_input = x\n","        x, down_x = self.encode(x)\n","        down_x.reverse()\n","\n","        vae_input = x\n","        x = self.decode(x, down_x)\n","\n","        if self.training:\n","            vae_reg_loss, vae_mse_loss = self._get_vae_loss(net_input, vae_input)\n","            return x, vae_reg_loss, vae_mse_loss\n","\n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Progress Bar :arrows_counterclockwise:"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.249082Z","iopub.status.busy":"2023-03-28T15:31:57.248342Z","iopub.status.idle":"2023-03-28T15:31:57.268244Z","shell.execute_reply":"2023-03-28T15:31:57.267159Z","shell.execute_reply.started":"2023-03-28T15:31:57.249043Z"},"trusted":true},"outputs":[],"source":["class BatchesProcessedColumn(ProgressColumn):\n","        def __init__(self, style: Union[str, Style]):\n","            self.style = style\n","            super().__init__()\n","\n","        def render(self, task: \"Task\") -> RenderableType:\n","            total = task.total if task.total != float(\"inf\") else \"--\"\n","            return Text(f\"{int(task.completed)}/{total}\", style=self.style)\n","\n","class CustomTimeColumn(ProgressColumn):\n","\n","        # Only refresh twice a second to prevent jitter\n","        max_refresh = 0.5\n","\n","        def __init__(self, style: Union[str, Style]) -> None:\n","            self.style = style\n","            self.start_time = datetime.now()\n","            super().__init__()\n","\n","        def render(self, task: \"Task\") -> Text:\n","            elapsed = task.finished_time if task.finished else task.elapsed\n","            remaining = task.time_remaining\n","            total = datetime.now() - self.start_time\n","            elapsed_delta = \"-:--:--\" if elapsed is None else str(timedelta(seconds=int(elapsed)))\n","            remaining_delta = \"-:--:--\" if remaining is None else str(timedelta(seconds=int(remaining)))\n","            return Text(f\"{elapsed_delta} • {remaining_delta} • {total.days}:{total.seconds // 3600}:{(total.seconds//60)%60}:{total.seconds%60}\", style=self.style)\n","\n","\n","class CustomRichProgressBar(RichProgressBar):\n","    \"\"\"\n","    Custom Progress Bar for PyTorch Lightning.\n","    Added the possibility to show the current steps and total steps in the progress bar.\n","    Added the elasped time from start training.\n","\n","    NOTE: in pytorch-lightning>=2.0 the RichProgressBar has undergone minor refactoring.\n","            Some names field have been changed to better distinguish the progress bar displayed during the different phases of the training.\n","    \"\"\"\n","\n","    def on_train_epoch_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -> None:\n","        if self.is_disabled:\n","            return\n","        total_batches = self.total_batches_current_epoch\n","        train_description = self._get_train_description(trainer.current_epoch)\n","\n","        if self.main_progress_bar_id is not None and self._leave:\n","            self._stop_progress()\n","            self._init_progress(trainer)\n","        if self.progress is not None:\n","            if self.main_progress_bar_id is None:\n","                self.main_progress_bar_id = self._add_task(total_batches, train_description)\n","            else:\n","                self.progress.reset(\n","                    self.main_progress_bar_id, total=total_batches, description=train_description, visible=True\n","                )\n","\n","        self.refresh()\n","    \n","    def _get_train_description(self, current_epoch: int) -> str:\n","        if self.trainer.max_epochs not in [None, -1]:\n","            train_description = f\"Epoch {current_epoch}\"\n","            if self.trainer.max_epochs is not None:\n","                train_description += f\"/{self.trainer.max_epochs - 1}\"\n","            if len(self.validation_description) > len(train_description):\n","                # Padding is required to avoid flickering due of uneven lengths of \"Epoch X\"\n","                # and \"Validation\" Bar description\n","                train_description = f\"{train_description:{len(self.validation_description)}}\"\n","            return train_description\n","        elif self.trainer.max_steps not in [None, -1]:\n","            train_description = f\"Step {self.trainer.global_step}\"\n","            if self.trainer.max_steps is not None:\n","                train_description += f\"/{self.trainer.max_steps - 1}\"\n","            if len(self.validation_description) > len(train_description):\n","                # Padding is required to avoid flickering due of uneven lengths of \"Epoch X\"\n","                # and \"Validation\" Bar description\n","                train_description = f\"{train_description:{len(self.validation_description)}}\"\n","            return train_description\n","        else:\n","            global max_time\n","            train_description = f\"Training Time: {max_time}\"\n","            return train_description\n","\n","\n","\n","\n","    def configure_columns(self, trainer: \"pl.Trainer\") -> list:\n","        return [\n","            TextColumn(\"[progress.description]{task.description}\"),\n","            CustomBarColumn(\n","                complete_style=self.theme.progress_bar,\n","                finished_style=self.theme.progress_bar_finished,\n","                pulse_style=self.theme.progress_bar_pulse,\n","            ),\n","            BatchesProcessedColumn(style=self.theme.batch_progress),\n","            CustomTimeColumn(style=self.theme.time),\n","            ProcessingSpeedColumn(style=self.theme.processing_speed),\n","        ]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Configs :notebook: "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.270717Z","iopub.status.busy":"2023-03-28T15:31:57.269925Z","iopub.status.idle":"2023-03-28T15:31:57.303448Z","shell.execute_reply":"2023-03-28T15:31:57.302502Z","shell.execute_reply.started":"2023-03-28T15:31:57.270679Z"},"trusted":true},"outputs":[],"source":["parser = argparse.ArgumentParser(description=\"Thesis Experiment\")\n","parser.add_argument('--default_config', '-c',\n","                    dest=\"default_config\",\n","                    metavar='FILE',\n","                    help=\"path to the default config file\",\n","                    default = config_dir + \"t_attunet_4_3_dicefocal.yaml\") # debug_vae_gpu_config mono0.yaml \n","\n","args, unknown = parser.parse_known_args() # add [] as argument\n","with open(args.default_config, 'r') as file:\n","    try:\n","        config = yaml.safe_load(file)\n","    except yaml.YAMLError as exc:\n","        print(exc)\n","\n","config"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.306962Z","iopub.status.busy":"2023-03-28T15:31:57.306666Z","iopub.status.idle":"2023-03-28T15:31:57.318485Z","shell.execute_reply":"2023-03-28T15:31:57.316112Z","shell.execute_reply.started":"2023-03-28T15:31:57.306920Z"},"trusted":true},"outputs":[],"source":["config = {'train_batch_size': 2,\n"," 'val_batch_size': 1,\n"," 'inChannels': 4,\n"," 'outClasses': 3,\n"," 'modelName': 'attentionunet',\n"," \n","'sweep_params': {'ntrials': 2,\n","  'project': 'MICCAI 2018 Medical Image Segmentation'},\n"," \n","'data_params': {'dataset': 'Task01_BrainTumour',\n","  'classes': 3,\n","  'data_path': '/kaggle/input/task01braintumour/',\n","  'train_batch_size': 2,\n","  'val_batch_size': 1,\n","  'num_workers': 2,\n","  'download': False,\n","  'cache_rate': 0.0},\n"," \n","'wandb_params': {'save_dir': '/kaggle/working/wandb',\n","  'manual_seed': 42,\n","  'ckpt_name': 'attentionunet',\n","  'model_name': 'attentionunet',\n","  'log_model': True},\n"," \n","'model_params': {'model_name': 'attentionunet',\n","  'feature_scale': 2,\n","  'in_channels': 4,\n","  'out_channels': 3,\n","  'is_deconv': True,\n","  'nonlocal_mode': 'concatenation',\n","  'attention_dsample': [2, 2, 2],\n","  'is_batchnorm': True,\n","  'dropout_prob': 0.2},\n"," \n","'criterion_params': {'criterion_name': 'dicefocal',\n","  'n_classes': 3,\n","  'squared_pred': True,\n","  'to_onehot_y': False,\n","  'sigmoid': True,\n","  'dice_reduction': 'none',\n","  'smooth_nr': 0,\n","  'smooth_dr': '1e-5',\n","  'gamma': 2.0,\n","  'lambda_dice': 0.8,\n","  'lambda_focal': 1.0},\n"," \n","'exp_params': {'deep_supervision': True,\n","  'use_hausdorff': True,\n","  'Hausdorff': {\n","      'alpha': 2.0,\n","      'k': 5,\n","      'reduction': 'mean'\n","  },\n","  'max_epochs': 25,\n","  'num_iters': 100,\n","  'inModalities': 4,\n","  'outClasses': 3,\n","  'train_batch_size': 2,\n","  'val_batch_size': 1,\n","  'adam': {'lr': 0.0008, 'weight_decay': 1e-05},\n","  'sgd': {'lr': 2e-05, 'momentum': 0.9, 'weight_decay': 0.01},\n","  'rmsprop': {'lr': 0.01,\n","   'momentum': 0.9,\n","   'alpha': 0.99,\n","   'weight_decay': 0.01}},\n"," \n","'trainer_params': {'accelerator': 'gpu', 'devices': 1},\n"," \n","'checkpoint_params': {'mode': 'min',\n","  'save_top_k': 1,\n","  'every_n_epochs': 1,\n","  'save_on_train_epoch_end': True},\n"," \n","'training_params': {'debug': False,\n","  'resume_train': False,\n","  'ckpt_path': None,\n","  'n_models_to_save': 2}}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Main :train2:"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.466762Z","iopub.status.busy":"2023-03-28T15:31:57.466306Z","iopub.status.idle":"2023-03-28T15:31:57.482745Z","shell.execute_reply":"2023-03-28T15:31:57.480945Z","shell.execute_reply.started":"2023-03-28T15:31:57.466723Z"},"trusted":true},"outputs":[],"source":["def main(config):\n","    \n","    \n","    # ------------------------\n","    # 1 WANDB LOGGER\n","    # ------------------------\n","    id = wandb.util.generate_id()\n","    print(\"Run id: \", id)\n","    \n","    wandb_logger = WandbLogger(\n","        project = 'MICCAI 2018 Medical Image Segmentation',\n","        save_dir = \"/kaggle/working/\",\n","        log_model = True, #\"all\": log while training\n","        config = config,\n","        resume = \"allow\",\n","        name = \"AttentionUnet - Train\",\n","        notes = \"BS: 2 + New DS + Both D + FD + FS: 2 + ReLU6 + a-Focal + HD_er\",\n","        #checkpoint_name = \"AttentionUnet\",\n","        id= id\n","    )\n","    \n","    # ------------------------\n","    # 2 INIT LIGHTNING MODEL\n","    # ------------------------\n","    model_name = wandb.config[\"model_params\"][\"model_name\"] #\"segresnetvae\"\n","    model = createModel(wandb.config['model_params'])\n","    \n","    #wandb_logger.watch(model, log_freq=480, log_graph=True) # every 500 steps; \n","    \n","    \n","    # ------------------------\n","    # 3 DATA PIPELINE\n","    # ------------------------\n","    wandb.config['data_params'].update(transform)\n","    \n","    data = DatasetModule(**wandb.config['data_params'])\n","    \n","    # ------------------------\n","    # 4 LIGHTNING EXPERIMENT\n","    # ------------------------\n","    criterion_name = wandb.config['criterion_params']['criterion_name']\n","    criterion = CreateLoss(wandb.config['criterion_params'])\n","    \n","    optimizer = \"adam\"\n","    \n","    experiment = ThesisExperiment(model_name, model, criterion_name, criterion, optimizer, wandb.config['exp_params'])\n","    \n","    # ------------------------\n","    # 5 TRAINER\n","    # ------------------------\n","        # 5.1 PROGRESSBAR\n","    # ----------------------\n","    theme = RichProgressBarTheme(description=\"white\",\n","                                progress_bar=\"blue\",\n","                                progress_bar_finished=\"green\",\n","                                progress_bar_pulse=\"bright_blue\",\n","                                batch_progress=\"white\",\n","                                time=\"gray54\",\n","                                processing_speed=\"gray70\",\n","                                metrics=\"white\")\n","    global max_time\n","    max_time = \"00:10:00:00\"\n","    \n","    \n","    progress_bar = CustomRichProgressBar(theme=theme)\n","    \n","    # add callbacks\n","    \n","    #ckpt_name = config['wandb_params']['ckpt_name']\n","    \n","    \n","    \n","    checkpointCBK = ModelCheckpoint(\n","            monitor = criterion_name + \"_loss_val\", #wandb.config['checkpoint_params']['monitor'],\n","            mode = wandb.config['checkpoint_params']['mode'],\n","            dirpath = os.path.join(wandb.config['wandb_params']['save_dir'], \"checkpoints\"),\n","            filename = model_name + '-{epoch}-{' + '{}_loss_val'.format(criterion_name) + ':.2f}',\n","            save_top_k = wandb.config['checkpoint_params']['save_top_k'],\n","            #train_time_interval = timedelta(minutes=5), #(hours=1),\n","            every_n_epochs = 1, #wandb.config['checkpoint_params']['every_n_epochs'],\n","            save_on_train_epoch_end= False #wandb.config['checkpoint_params']['save_on_train_epoch_end']\n","        )\n","    \n","    lrMonitorCBK = LearningRateMonitor(logging_interval='epoch')\n","    \n","    callbacks = [\n","        checkpointCBK,\n","        lrMonitorCBK,\n","        progress_bar\n","    ]\n","    \n","    doResume= False\n","    ckpt_file = \"/kaggle/working/AttentionUnet_epoch_25_aFocal.ckpt\"\n","    \n","    trainer = Trainer(\n","            accelerator = 'gpu',\n","            devices = 1,\n","            logger= wandb_logger,\n","            callbacks = callbacks,\n","            fast_dev_run = False,\n","            enable_checkpointing = True,\n","            benchmark=True,\n","            log_every_n_steps = 50,\n","            max_epochs = 25, #10\n","            check_val_every_n_epoch = 1,\n","            #val_check_interval = 1,\n","            max_time = max_time,\n","            precision = 16,\n","            limit_train_batches = 1.0,\n","            limit_val_batches = 0.5,\n","            #**wandb.config['trainer_params']\n","        )\n","    \n","    \n","    \n","    # ------------------------\n","    # 6 START TRAINING\n","    # ------------------------\n","    print(\"\\nStart training!\\n\")\n","    trainer.fit(experiment, datamodule=data, ckpt_path = ckpt_file if doResume else None)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-28T15:31:57.484599Z","iopub.status.busy":"2023-03-28T15:31:57.484204Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Run id:  1n6vpurq\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmattiacapparella\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.14.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230328_153200-1n6vpurq</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/mattiacapparella/MICCAI%202018%20Medical%20Image%20Segmentation/runs/1n6vpurq\" target=\"_blank\">AttentionUnet - Train</a></strong> to <a href=\"https://wandb.ai/mattiacapparella/MICCAI%202018%20Medical%20Image%20Segmentation\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Start training!\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning:\n","\n","Checkpoint directory /kaggle/working/wandb/checkpoints exists and is not empty.\n","\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n","┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n","┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model          │ MultiAttentionUnet │ 25.9 M │\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ val_dice       │ Val_Dice           │      0 │\n","│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ hausdorff_loss │ HausdorffERLoss3D  │      0 │\n","└───┴────────────────┴────────────────────┴────────┘\n","</pre>\n"],"text/plain":["┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n","┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n","┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n","│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model          │ MultiAttentionUnet │ 25.9 M │\n","│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ val_dice       │ Val_Dice           │      0 │\n","│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ hausdorff_loss │ HausdorffERLoss3D  │      0 │\n","└───┴────────────────┴────────────────────┴────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 25.9 M                                                                                           \n","<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n","<span style=\"font-weight: bold\">Total params</span>: 25.9 M                                                                                               \n","<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 51                                                                         \n","</pre>\n"],"text/plain":["\u001b[1mTrainable params\u001b[0m: 25.9 M                                                                                           \n","\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n","\u001b[1mTotal params\u001b[0m: 25.9 M                                                                                               \n","\u001b[1mTotal estimated model params size (MB)\u001b[0m: 51                                                                         \n"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13b2fb0aed1c44479fc985aa4f35322f","version_major":2,"version_minor":0},"text/plain":["Output()"]},"metadata":{},"output_type":"display_data"}],"source":["if __name__ == '__main__':\n","    \n","    main(config)\n","    print(\"\\nEnd training!\\n\")\n","    wandb.finish()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
